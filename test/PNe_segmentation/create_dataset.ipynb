{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparación del entorno\n",
    "Para empezar, lo primero que vamos a hacer va a ser preparar el entorno en el que se van a hacer las ejecuciones, de donde el programa tiene que cargar las imágenes y las máscaras de las nebulosas planetarias y donde va a guardar los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Vamos a cambiar el directorio de trabajo\")\n",
    "\n",
    "# Indicamos la ruta del directorio de trabajo\n",
    "# route = \"C:\\\\Users\\\\Lucan\\\\OneDrive - Universidade da Coruña\\\\Escritorio\\\\4_GCEID\\\\TFG\\\\test\\\\PNe_segmentation\"\n",
    "route = os.getcwd() #+ \"/TFG/test/PNe_segmentation\"\n",
    "os.chdir(route)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"\\nEl directorio actual es:\", current_directory)\n",
    "\n",
    "# Listamos el contenido del directorio\n",
    "files = os.listdir(current_directory)\n",
    "print(\"\\nContenido del directorio actual:\")\n",
    "for file in files:\n",
    "    print(\"\\t\",file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Carga de las imágenes junto a sus máscaras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer una prueba de como cargaríamos una máscara y contorno junto con la imagen a la que pertenece. Pero antes vamos a realizar ciertas operaciones para tener almacenados y clasificados todos los archivos que tenemos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listamos el contenido del directorio de las máscaras\n",
    "# masks_directory = \"C:\\\\Users\\\\Lucan\\\\OneDrive - Universidade da Coruña\\\\Escritorio\\\\4_GCEID\\\\TFG\\\\test\\\\PNe_segmentation\\\\masks\"\n",
    "masks_directory = os.getcwd() + \"/masks\"\n",
    "masksFiles = os.listdir(masks_directory)\n",
    "masks_files = [file for file in masksFiles if file.endswith(\".png\")]\n",
    "\n",
    "# data_directory = \"C:\\\\Users\\\\Lucan\\\\OneDrive - Universidade da Coruña\\\\Escritorio\\\\4_GCEID\\\\TFG\\\\test\\\\PNe_segmentation\\\\data\"\n",
    "data_directory = os.getcwd() + \"/data\"\n",
    "dataFiles = os.listdir(data_directory)\n",
    "data_files = [file for file in dataFiles if file.endswith(\".fits\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un diccionario en el que las claves sean el nombre identificador de la nebulosa y el valor sea una lista con las máscaras y contornos disponibles de esa nebulosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_dict = {}\n",
    "\n",
    "for mask_file in masks_files:\n",
    "    nebula_name = mask_file.split(\"_\")[:-1]\n",
    "    nebula_name = \"_\".join(nebula_name)\n",
    "    if masks_dict.get(nebula_name) is None:\n",
    "        masks_dict[nebula_name] = [mask_file]\n",
    "    else: \n",
    "        masks_dict[nebula_name].append(mask_file)\n",
    "\n",
    "masks_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que todas las máscaras/contornos disponibles se han asignado a una nebulosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_dict_count = sum(len(files) for files in masks_dict.values())\n",
    "masks_files_count = len(masks_files)\n",
    "\n",
    "if masks_dict_count == masks_files_count:\n",
    "    print(\"The number of files in masks_dict is the same as the number of files in masks_files.\")\n",
    "else:\n",
    "    print(\"The number of files in masks_dict is different from the number of files in masks_files.\")\n",
    "    \n",
    "    files_not_in_data_dict = set(masks_files) - set(sum(masks_dict.values(), []))\n",
    "    print(\"Files not in data_dict:\", files_not_in_data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un diccionario similar al anterior, con las mismas claves, pero en los valores se van a guardar los archivos fits que se refieren a la nebulosa indicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data_dict = {}\n",
    "\n",
    "for nebula in masks_dict.keys():\n",
    "    patron = re.compile(r'(?i)' + nebula + r'\\D+.*')\n",
    "    data_dict[nebula] = [file for file in data_files if patron.match(file)]\n",
    "\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que todos los archivos fits disponibles se han asignado a una nebulosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_count = sum(len(files) for files in data_dict.values())\n",
    "data_files_count = len(data_files)\n",
    "\n",
    "if data_dict_count == data_files_count:\n",
    "    print(\"The number of files in data_dict is the same as the number of files in data_files.\")\n",
    "else:\n",
    "    print(\"The number of files in data_dict is different from the number of files in data_files.\")\n",
    "    \n",
    "    files_not_in_data_dict = set(data_files) - set(sum(data_dict.values(), []))\n",
    "    print(\"Files not in data_dict:\", files_not_in_data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descartamos la nebulosa K3_72 debido a que hemos realizado dos segmentaciones las cuales en un futuro tendremos que decidir con cual nos quedamos o utilizar las dos de alguna manera (como si fueran dos nebulosas independientes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict.pop('K3_72_big', None)\n",
    "data_dict.pop('K3_72_small', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto tenemos dos diccionarios con los cuales cargar los archivos facilmente y clasificados por el nombre identificador de la nebulosa.\n",
    "Ahora sí, vamos a cargar todos los canales disponibles de una nebulosa junto a su contorno y máscara (si tiene)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import imageio as io\n",
    "import numpy as np\n",
    "def cargar_canales_nebulosa(nebula, data_dict, masks_dict, data_directory, masks_directory, name_files:bool=False):\n",
    "    data_files = data_dict[nebula]\n",
    "    masks_files = masks_dict[nebula]\n",
    "    data = {}\n",
    "    masks = {}\n",
    "    charge_files = {}\n",
    "    \n",
    "    for file in masks_files:\n",
    "        key = file.split(\"_\")[-1].split(\".\")[0]\n",
    "        image = io.imread(\"masks/\"+file)\n",
    "        if len(image.shape) > 2:\n",
    "            image = image[:,:,0]\n",
    "        masks[key] = image\n",
    "        \n",
    "    for file in data_files:\n",
    "        image = fits.getdata(\"data/\"+file)\n",
    "        image = np.flip(image, axis=0)\n",
    "        if image.shape == masks['contour'].shape:\n",
    "            key = file.replace(nebula, \"\").replace(\"_\",\"\")[0].lower()\n",
    "            data[key] = image\n",
    "            charge_files[key] = file\n",
    "        # else:\n",
    "        #     print(f\"Error: {file} has a different shape than the masks.\")\n",
    "    if not name_files:\n",
    "        return data, masks\n",
    "    else:\n",
    "        return data, masks, charge_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, ya podríamos cargar todos los canales y máscaras que tenemos, pudiendo distinguir por nebulosa. Por ejemplo, en la siguiente celda de código vamos a cargar todos los canales de una nebulosa en específico junto a sus máscaras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['K2_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canales, mascaras = cargar_canales_nebulosa(\"K3_46\", data_dict, masks_dict, data_directory, masks_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a analizar como son las imagenes obtenidas, sus valores mínimos y máximos, sus dimensiones, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_info_imagen(image, title):\n",
    "    min_value = np.min(image)\n",
    "    max_value = np.max(image)\n",
    "    dimensions = image.shape\n",
    "    null_values = np.isnan(image).sum()\n",
    "    print(title)\n",
    "    print(f\"Minimum value: {min_value}\")\n",
    "    print(f\"Maximum value: {max_value}\")\n",
    "    print(f\"Null values: {null_values}\")\n",
    "    print(f\"Dimensions: {dimensions}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze canales' images\n",
    "for key, channel in canales.items():\n",
    "    imprimir_info_imagen(channel, f\"Channel {key}:\")\n",
    "\n",
    "# Analyze mascaras' images\n",
    "for key, mask in mascaras.items():\n",
    "    imprimir_info_imagen(mask, f\"Mask {key}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_images = len(canales)\n",
    "\n",
    "fig, ax = plt.subplots(1, num_images, figsize=(10*num_images, 10))\n",
    "fig.suptitle(f\"Canales de la nebulosa A2\", fontweight = 'bold', fontsize = 14)\n",
    "\n",
    "for i, (key, channel) in enumerate(canales.items()):\n",
    "    ax[i].imshow(channel, cmap = \"gray\")\n",
    "    ax[i].set_title(f\"Canal {key}\")\n",
    "fig.show()\n",
    "\n",
    "if 'mask' in mascaras.keys():\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    fig.suptitle(f\"Contorno y máscara de la nebulosa A2\", fontweight = 'bold', fontsize = 14)\n",
    "\n",
    "    ax[0].imshow(mascaras['contour'], cmap = \"gray\")\n",
    "    ax[1].imshow(mascaras['mask'], cmap = \"gray\")\n",
    "    fig.show()\n",
    "else:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    fig.suptitle(f\"Contorno de la nebulosa A2\", fontweight = 'bold', fontsize = 14)\n",
    "\n",
    "    ax.imshow(mascaras['contour'], cmap = \"gray\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Preprocesado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como parte del preprocesado inicial, vamos a hacer un estudio de cuales son los canales/filtros que comparten todas (o la mayoría de nebulosas) para así empezar a hacer pruebas con un conjunto de imágenes lo más homogéneo posible (para los canales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_dict.keys():\n",
    "    canales, mascaras = cargar_canales_nebulosa(i, data_dict, masks_dict, data_directory, masks_directory)\n",
    "    if len(canales) != 2:\n",
    "        print(i, len(canales))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto se han eliminado algunas imágenes de nebulosas del conjunto de datos:\n",
    "- A33: dimensiones descuadradas\n",
    "- H3_75: dimensiones descuadradas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comprobar que las nebulosas tengan mínimo un canal en común y ver cual es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = {}\n",
    "for i in data_dict.keys():\n",
    "    canales, mascaras = cargar_canales_nebulosa(i, data_dict, masks_dict, data_directory, masks_directory)\n",
    "    for key in canales.keys():\n",
    "        if key in cnt.keys():\n",
    "            cnt[key] += 1\n",
    "        else:\n",
    "            cnt[key] = 1\n",
    "        \n",
    "        if key == '.':\n",
    "            print(i, key, canales.keys())\n",
    "\n",
    "print(cnt)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descartamos la nebulosa h2 debido a que no se no sabemos a que canal pertenece (creo que es una combinación en escala de grises de diferentes canales por como se describe en el .fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict.pop('h2', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, la mayoría de nebulosas tienen el canal 'h', por lo que nos vamos a quedar con todas las imágenes que contiene ese canal y vamos a empezar a trabajar con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dict = {}\n",
    "for i in data_dict.keys():\n",
    "    canales, mascaras, charge_files = cargar_canales_nebulosa(i, data_dict, masks_dict, data_directory, masks_directory, True)\n",
    "    if 'h' in canales.keys():\n",
    "        if 'n' in canales.keys() or 'o' in canales.keys() or 'b' in canales.keys():\n",
    "            charge_files.pop('n', None)\n",
    "            charge_files.pop('o', None)\n",
    "            charge_files.pop('b', None)\n",
    "        new_data_dict[i] = charge_files\n",
    "\n",
    "print(f\"El tamaño de data_dict es {len(data_dict)} y el tamaño de new_data_dict es {len(new_data_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear el diccionario definitivo con los archivos que vamos a cargar. Para esta prueba inicial, vamos a cargar como máscara única el archivo _mask si la nebulosa lo tiene y si no cargaremos el _contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nebula, files in new_data_dict.items():\n",
    "    mask = [file for file in masks_dict[nebula] if 'contour' in file][0]\n",
    "    if len(masks_dict[nebula]) > 1:\n",
    "        mask = [file for file in masks_dict[nebula] if 'mask' in file][0]\n",
    "    new_data_dict[nebula].update({'mask': mask})\n",
    "# Mostramos un ejemplo para ver que todo ha ido correctamente\n",
    "print(new_data_dict[nebula])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a transformarlo a un dataframe de pandas para poder exportarlo a .csv y más adelante no tener que volver a hacer todo el procedimiento, además trabajar con dataframes de pandas puede ser mucho más accesible y fácil de visualizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(new_data_dict, orient='index')\n",
    "\n",
    "# Quiero cargar el nombre de la nebulosa como una columna y cambiarle el nombre de index a name\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index':'name'}, inplace=True)\n",
    "print(df.head())\n",
    "\n",
    "# Guardar el dataframe en un archivo csv\n",
    "df.to_csv(\"data_files_1c.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegados a este punto, a no ser que se amplie el dataset de imágenes, no vamos a tener que volver a ejecutar esta parte inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3. División de los conjuntos train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a formar dos .csv a partir del principal (creado anteriormente), uno que solo tenga las nebulosas usadas para entrenamiento y otro que solo tenga las nebulosas para test. De este modo, cuando ampliamos el dataset con más nebulosas, solo tenemos que utilizar el 'dataset_info.csv' para que siempre las mismas imágenes se encuentren en entrenamiento y en test, para que a la hora de realizar las pruebas con los distintos algoritmos no haya 'contaminaciones'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_general = pd.read_csv(\"data_files_1c.csv\")\n",
    "df_info = pd.read_csv(\"dataset_info.csv\", sep=';')\n",
    "\n",
    "df_merged = pd.merge(df_general, df_info, on='name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>complete_tag</th>\n",
       "      <th>simple_tag</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A33</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>K3_72</td>\n",
       "      <td>Br</td>\n",
       "      <td>B</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name complete_tag simple_tag    set\n",
       "9     A33            R          R   test\n",
       "99  K3_72           Br          B  train"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_rows = df_info[~df_info['name'].isin(df_general['name'])]\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos las divisiones de train y de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_merged[df_merged['set'] == 'train']\n",
    "df_test = df_merged[df_merged['set'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que la suma del número de filas de ambas particiones más las filas que se han perdido por el camino suman el número total de filas en df_info (nebulosas procesadas, aunque algunas se hayan perdido por el camino por diversas razones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape[0] + df_test.shape[0] + missing_rows.shape[0] == df_info.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos en dos .csv distintos los conjuntos de train y de test, con sus etiquetas incluidas y todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['set'])\n",
    "df_test = df_test.drop(columns=['set'])\n",
    "\n",
    "df_train.to_csv(\"data_files_1c_train.csv\", index=False)\n",
    "df_test.to_csv(\"data_files_1c_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
