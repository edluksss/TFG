{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento personalizado de YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Vamos a cambiar el directorio de trabajo\")\n",
    "\n",
    "# Indicamos la ruta del directorio de trabajo\n",
    "route = os.getcwd()+\"/TFG/test/PNe_segmentation\"\n",
    "os.chdir(route)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\" El directorio actual es:\", current_directory)\n",
    "\n",
    "# Listamos el contenido del directorio\n",
    "files = os.listdir(current_directory)\n",
    "print(\" Contenido del directorio actual:\")\n",
    "for file in files:\n",
    "    print(\"\\t\",file)\n",
    "    \n",
    "# Listamos el contenido del directorio de las máscaras\n",
    "# masks_directory = route+\"TFG\\\\test\\\\PNe_segmentation\\\\masks\"\n",
    "# data_directory = route+\"TFG\\\\test\\\\PNe_segmentation\\\\data\"\n",
    "## Ejecución en el CESGA Finisterrae III\n",
    "masks_directory = current_directory+\"/masks\"\n",
    "data_directory = current_directory+\"/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionamos el modelo from scratch para saber la entrada y la salida que espera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Create a new YOLO model from scratch\n",
    "model = YOLO(\"yolov8n-seg.yaml\", task='segment', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificamos el modelo para que en vez de recibir 3 capas de entrada reciba uno solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a la capa de convolución inicial\n",
    "conv_layer = model.model.model[0].conv\n",
    "\n",
    "# Crear una nueva capa de convolución con 1 canal de entrada en lugar de 3\n",
    "new_conv_layer = torch.nn.Conv2d(1, conv_layer.out_channels, kernel_size=conv_layer.kernel_size, stride=conv_layer.stride, padding=conv_layer.padding, bias=conv_layer.bias is not None)\n",
    "\n",
    "# Reemplazar la capa de convolución en el modelo\n",
    "model.model.model[0].conv = new_conv_layer\n",
    "\n",
    "# Verificar la nueva configuración de la capa\n",
    "print(model.model.model[0])\n",
    "\n",
    "# # Acceder a la capa de convolución inicial\n",
    "# conv_layer = model.model.model[0].conv\n",
    "# bn_layer = model.model.model[0].bn\n",
    "# act_layer = model.model.model[0].act\n",
    "\n",
    "# # Crear una nueva capa de convolución con 1 canal de entrada en lugar de 3\n",
    "# new_conv_layer = torch.nn.Conv2d(1, 3, kernel_size=1, padding='same', bias=False)\n",
    "\n",
    "# new_first_block = torch.nn.Sequential(new_conv_layer, conv_layer, bn_layer, act_layer)\n",
    "\n",
    "# # Reemplazar la capa de convolución en el modelo\n",
    "# model.model.model[0] = new_first_block\n",
    "\n",
    "# # Verificar la nueva configuración de la capa\n",
    "# print(model.model.model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        self.f = -1\n",
    "        self.i = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_f(self):\n",
    "        return self.f\n",
    "    \n",
    "    def set_f(self, f):\n",
    "        self.f = f\n",
    "        \n",
    "model.model.model.append(MyModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya tenemos el modelo modificado, vamos a probar a cargarlo con nuestro Lightning Module personalizado para entrenarlo a nuestro manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnebulae_torch.models import smpAdapter\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "model = model.model\n",
    "conv_model = smpAdapter(model = model, learning_rate=0.0001, threshold=0.5, current_fold=0, loss_fn=smp.losses.DiceLoss, scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.preprocess import CutValues\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "transform_x = transforms.Compose([\n",
    "                    # MinMaxNorm,\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    # MinMaxImageNorm(min = -88.9933, max=125873.7500),\n",
    "                    # ApplyMorphology(operation = morphology.binary_opening, concat = True, footprint = morphology.disk(2)),\n",
    "                    # ApplyMorphology(operation = morphology.area_opening, concat = True, area_threshold = 200, connectivity = 1),\n",
    "                    # ApplyIntensityTransformation(transformation = exposure.equalize_hist, concat = True, nbins = 4096),\n",
    "                    # ApplyIntensityTransformation(transformation = exposure.equalize_adapthist, concat = True, nbins = 640, kernel_size = 5),\n",
    "                    # ApplyMorphology(operation = morphology.area_opening, concat = True, area_threshold = 200, connectivity = 1),\n",
    "                    # ApplyFilter(filter = ndimage.gaussian_filter, concat = True, sigma = 5),\n",
    "                    # transforms.ToTensor(),\n",
    "                    # CustomPad(target_size = (1984, 1984), fill_min=True, tensor_type=torch.Tensor.float)\n",
    "                    # ApplyIntensityTransformation(transformation = exposure.equalize_hist, concat = False, nbins = 256),\n",
    "                    transforms.ToTensor(),\n",
    "                    ])\n",
    "type_fnc = torch.Tensor.int\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Lambda(lambda x: type_fnc(x.round())),\n",
    "                    # CustomPad(target_size = (1984, 1984), fill = 0, tensor_type=torch.Tensor.int)\n",
    "                    ])\n",
    "\n",
    "df_train = pd.read_csv(\"data_files_1c_train.csv\")\n",
    "dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = conv_model(dataset_train[156][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(torch.sigmoid(a)[0][0].detach().numpy() > 0.55, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a intentar hacer un entrenamiento al igual que hacemos para el resto de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.preprocess import ApplyMorphology, ApplyIntensityTransformation, ApplyFilter, CustomPad, CutValues\n",
    "from pnebulae_torch.normalize import TypicalImageNorm, MinMaxImageNorm\n",
    "from pnebulae_torch.models.callbacks import PrintCallback\n",
    "from pnebulae_torch.models import basicUNet, smpAdapter, ConvNet\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "from skimage import morphology, exposure\n",
    "from scipy import ndimage\n",
    "from lightning.pytorch import seed_everything\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import wandb\n",
    "import inspect\n",
    "import time\n",
    "import gc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ########## CONFIGURACIÓN SCRIPT ##########\n",
    "    # Establecemos la clave de la API de W&B\n",
    "    os.environ[\"WANDB_API_KEY\"] = \"21924e6e134841c5c16842c4ac42fcbe5a66feb2\"\n",
    "    ruta_logs_wandb = os.environ[\"STORE\"] + \"/TFG/logs_wandb/\"\n",
    "    \n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "    ####### CONFIGURACIÓN ENTRENAMIENTO #######\n",
    "    model_name = \"YOLO_test\"\n",
    "    \n",
    "    BATCH_SIZE = 124\n",
    "    num_epochs = 2000\n",
    "    lr = 1e-2\n",
    "    window_shape = 640\n",
    "    \n",
    "    k = 5\n",
    "    \n",
    "    loss_fn = DiceLoss\n",
    "    activation_layer=torch.nn.ReLU\n",
    "    \n",
    "    if \"mode\" in inspect.signature(loss_fn).parameters:\n",
    "        type_fnc = torch.Tensor.int\n",
    "    else:\n",
    "        type_fnc = torch.Tensor.float\n",
    "        \n",
    "    ############# CARGA DATASET #############\n",
    "    transform_x = transforms.Compose([\n",
    "                        # MinMaxNorm,\n",
    "                        CutValues(factor = 2),\n",
    "                        TypicalImageNorm(factor = 1, substract=0),\n",
    "                        # MinMaxImageNorm(min = -88.9933, max=125873.7500),\n",
    "                        # ApplyMorphology(operation = morphology.binary_opening, concat = True, footprint = morphology.disk(2)),\n",
    "                        # ApplyMorphology(operation = morphology.area_opening, concat = True, area_threshold = 200, connectivity = 1),\n",
    "                        # ApplyIntensityTransformation(transformation = exposure.equalize_hist, concat = True, nbins = 4096),\n",
    "                        # ApplyIntensityTransformation(transformation = exposure.equalize_adapthist, concat = True, nbins = 640, kernel_size = 5),\n",
    "                        # ApplyMorphology(operation = morphology.area_opening, concat = True, area_threshold = 200, connectivity = 1),\n",
    "                        # ApplyFilter(filter = ndimage.gaussian_filter, concat = True, sigma = 5),\n",
    "                        # transforms.ToTensor(),\n",
    "                        # CustomPad(target_size = (1984, 1984), fill_min=True, tensor_type=torch.Tensor.float)\n",
    "                        # ApplyIntensityTransformation(transformation = exposure.equalize_hist, concat = False, nbins = 256),\n",
    "                        transforms.ToTensor(),\n",
    "                        ])\n",
    "\n",
    "    transform_y = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Lambda(lambda x: type_fnc(x.round())),\n",
    "                        # CustomPad(target_size = (1984, 1984), fill = 0, tensor_type=torch.Tensor.int)\n",
    "                        ])\n",
    "\n",
    "    df_train = pd.read_csv(\"data_files_1c_train.csv\")\n",
    "    dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))\n",
    "    \n",
    "    df_test = pd.read_csv(\"data_files_1c_test.csv\")\n",
    "    dataset_test = NebulaeDataset(data_directory, masks_directory, df_test, transform = (transform_x, transform_y))\n",
    "\n",
    "    seed_everything(42, workers = True)\n",
    "    \n",
    "    ########## ENTRENAMIENTO MODELO ##########\n",
    "    # Definimos el K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state = 42)\n",
    "    \n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset_train)):\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_loss',\n",
    "            dirpath=os.environ[\"STORE\"] + f\"/TFG/model_checkpoints/{model_name}\",\n",
    "            filename='best_model-{epoch:02d}-'+str(fold),\n",
    "            save_top_k=1,\n",
    "            mode='min',\n",
    "        )\n",
    "        \n",
    "        checkpoint_callback_last = ModelCheckpoint(\n",
    "            monitor=None,\n",
    "            dirpath=os.environ[\"STORE\"] + f\"/TFG/model_checkpoints/{model_name}\",\n",
    "            filename='last_model_fold'+str(fold),\n",
    "        )\n",
    "        \n",
    "        callbacks = [PrintCallback(), LearningRateMonitor(logging_interval='epoch'), checkpoint_callback, checkpoint_callback_last]\n",
    "        \n",
    "        # Acceder a la capa de convolución inicial\n",
    "        model = YOLO(\"yolov8n-seg.yaml\", task='segment')\n",
    "        \n",
    "        conv_layer = model.model.model[0].conv\n",
    "\n",
    "        # Crear una nueva capa de convolución con 1 canal de entrada en lugar de 3\n",
    "        new_conv_layer = torch.nn.Conv2d(dataset_train[0][0].shape[0], conv_layer.out_channels, kernel_size=conv_layer.kernel_size, stride=conv_layer.stride, padding=conv_layer.padding, bias=conv_layer.bias is not None)\n",
    "\n",
    "        # Reemplazar la capa de convolución en el modelo\n",
    "        model.model.model[0].conv = new_conv_layer\n",
    "\n",
    "        class MyModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(MyModel, self).__init__()\n",
    "                self.model = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n",
    "                    torch.nn.Upsample(scale_factor=2),\n",
    "                    torch.nn.Upsample(scale_factor=2)\n",
    "                )\n",
    "                self.f = -1\n",
    "                self.i = 0\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.model(x)\n",
    "            \n",
    "            def get_f(self):\n",
    "                return self.f\n",
    "            \n",
    "            def set_f(self, f):\n",
    "                self.f = f\n",
    "                \n",
    "        model.model.model.append(MyModel())\n",
    "        # # Acceder a la capa de convolución inicial\n",
    "        # conv_layer = model.model.model[0].conv\n",
    "        # bn_layer = model.model.model[0].bn\n",
    "        # act_layer = model.model.model[0].act\n",
    "\n",
    "        # # Crear una nueva capa de convolución con 1 canal de entrada en lugar de 3\n",
    "        # new_conv_layer = torch.nn.Conv2d(1, 3, kernel_size=1, padding='same', bias=False)\n",
    "\n",
    "        # new_first_block = torch.nn.Sequential(new_conv_layer, conv_layer, bn_layer, act_layer)\n",
    "\n",
    "        # # Reemplazar la capa de convolución en el modelo\n",
    "        # model.model.model[0] = new_first_block\n",
    "        \n",
    "        model = model.model\n",
    "        # Definimos el modelo con los pesos inicializados aleatoriamente (sin preentrenar)\n",
    "        # model = smpAdapter(model = model, learning_rate=lr, threshold=0.5, current_fold=fold, loss_fn=loss_fn, scheduler=None)\n",
    "        # model = smpAdapter(model = model, learning_rate=lr, threshold=0.5, current_fold=fold, loss_fn=loss_fn, scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau, mode='min', factor=0.1, patience=500, cooldown=150, verbose=False)\n",
    "        model = smpAdapter(model = model, learning_rate=lr, threshold=0.5, current_fold=fold, loss_fn=loss_fn, scheduler=torch.optim.lr_scheduler.StepLR, step_size = 500, gamma = 0.1, verbose=False)\n",
    "        # model = smpAdapter(model = model, learning_rate=lr, threshold=0.5, current_fold=fold, loss_fn=loss_fn, scheduler=torch.optim.lr_scheduler.MultiStepLR, milestones = [1000, 4000], gamma = 0.1, verbose=False)\n",
    "        \n",
    "        ruta_logs_wandb = os.environ[\"STORE\"] + \"/TFG/logs_wandb/\"\n",
    "        logger_wandb = WandbLogger(project=\"segmentation_TFG\", log_model = False, name=model_name, save_dir=ruta_logs_wandb)\n",
    "        logger_wandb.experiment.config.update({\"model_name\": model_name})\n",
    "\n",
    "        # log gradients, parameter histogram and model topology\n",
    "        logger_wandb.watch(model, log=\"all\")\n",
    "\n",
    "        trainer = L.Trainer(strategy='auto', max_epochs=num_epochs, accelerator='cuda', log_every_n_steps=2, logger= logger_wandb, callbacks=callbacks)\n",
    "\n",
    "        # Imprimimos el fold del que van a mostrarse los resultados\n",
    "        print('--------------------------------')\n",
    "        print(f\"Model info:\\n\\t- Batch Size: {BATCH_SIZE}\\n\\t- GPUs on use: {torch.cuda.device_count()}\")\n",
    "\n",
    "        # Creamos nuestros propios Subsets de PyTorch aplicando a cada conjunto la transformacion deseada\n",
    "        train_subset = torch.utils.data.Subset(dataset_train, train_ids)\n",
    "        val_subset = torch.utils.data.Subset(dataset_train, val_ids)\n",
    "        \n",
    "        if window_shape is not None:\n",
    "            train_subset = DivideWindowsSubset(train_subset, window_shape = window_shape, fill_min = True)\n",
    "            val_subset = DivideWindowsSubset(val_subset, window_shape = window_shape, fill_min = True)\n",
    "        \n",
    "        # Definimos un data loader por cada conjunto de datos que vamos a utilizar.\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                                train_subset,\n",
    "                                batch_size=BATCH_SIZE, num_workers=6, shuffle=True, persistent_workers=False)\n",
    "\n",
    "        valloader = torch.utils.data.DataLoader(\n",
    "                                val_subset,\n",
    "                                batch_size=BATCH_SIZE, num_workers=6, shuffle=False, persistent_workers=False)\n",
    "        \n",
    "        # Entrenamos el modelo, extrayendo los resultados y guardandolos en la variable result, y evaluamos en el conjunto de test.\n",
    "        trainer.fit(model, trainloader, valloader) \n",
    "\n",
    "        logger_wandb.experiment.unwatch(model)\n",
    "\n",
    "        # testloader = torch.utils.data.DataLoader(\n",
    "        #                         dataset_test,\n",
    "        #                         batch_size=BATCH_SIZE, num_workers=8, shuffle=False, persistent_workers=True)\n",
    "        \n",
    "        # Creamos un nuevo entrenador con una sola GPU para la fase de prueba\n",
    "        # trainer_test = L.Trainer(devices = 1, strategy='auto', max_epochs=num_epochs, accelerator='cuda', log_every_n_steps=1, logger=logger_wandb, callbacks=callbacks)\n",
    "        # trainer_test.test(model, testloader)\n",
    "\n",
    "        logger_wandb.finalize(\"success\")\n",
    "        wandb.finish()\n",
    "        \n",
    "        del model\n",
    "        del trainer\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento por defecto de YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este punto hemos conseguido entrenar el modelo YOLO, con una cabeza final personalizada y empleando nuestro método de entrenamiento, lo que vamos a proabr ahora es a entrenar YOLO tal y como viene preparado en la librería, pero vamos a seguir intentando hacer el 5Fold-CrossValidation e introduciendo las mismas imágenes que hemos probado para el resto de modelos. Para ellos lo que vamos a hacer es guardar todas las imágenes, en las ventanas de 512 respectivas y para cada fold las vamos a ir moviendo entre las diferentes carpetas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamos a cambiar el directorio de trabajo\n",
      " El directorio actual es: /mnt/netapp2/Home_FT2/home/ulc/co/ela/TFG/test/PNe_segmentation\n",
      " Contenido del directorio actual:\n",
      "\t train_models\n",
      "\t data\n",
      "\t create_dataset.ipynb\n",
      "\t data_files_1c.csv\n",
      "\t images\n",
      "\t image_analysis.ipynb\n",
      "\t masks\n",
      "\t segmentation_no_supervisada.ipynb\n",
      "\t segmentation_no_supervisada_2c.ipynb\n",
      "\t segmentation_no_supervisada_pytorch.ipynb\n",
      "\t segmentation_supervisada.ipynb\n",
      "\t dataset_info.csv\n",
      "\t data_files_1c_train.csv\n",
      "\t historico_notebooks\n",
      "\t data_files_1c_test.csv\n",
      "\t segmentation_no_supervisada_pytorch_clean.ipynb\n",
      "\t segmentation_supervisada_pytorch copy.ipynb\n",
      "\t segmentation_supervisada_pytorch.ipynb\n",
      "\t cesga\n",
      "\t segmentation_supervisada_pytorch copy 2.ipynb\n",
      "\t segmentation_supervisada_pytorch copy 3.ipynb\n",
      "\t data_files_1c_train_da.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Vamos a cambiar el directorio de trabajo\")\n",
    "\n",
    "# Indicamos la ruta del directorio de trabajo\n",
    "route = os.getcwd()+\"/TFG/test/PNe_segmentation\"\n",
    "os.chdir(route)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\" El directorio actual es:\", current_directory)\n",
    "\n",
    "# Listamos el contenido del directorio\n",
    "files = os.listdir(current_directory)\n",
    "print(\" Contenido del directorio actual:\")\n",
    "for file in files:\n",
    "    print(\"\\t\",file)\n",
    "    \n",
    "# Listamos el contenido del directorio de las máscaras\n",
    "# masks_directory = route+\"TFG\\\\test\\\\PNe_segmentation\\\\masks\"\n",
    "# data_directory = route+\"TFG\\\\test\\\\PNe_segmentation\\\\data\"\n",
    "## Ejecución en el CESGA Finisterrae III\n",
    "masks_directory = current_directory+\"/masks\"\n",
    "data_directory = current_directory+\"/data\"\n",
    "\n",
    "os.chdir(route+\"/../yolo_segmentation\")\n",
    "save_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir fits a JPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo realizamos para el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from pnebulae_torch.preprocess import CutValues\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from skimage import exposure\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "transform_x = transforms.Compose([\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "df_train = pd.read_csv(route+\"/data_files_1c_train.csv\")\n",
    "dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))\n",
    "\n",
    "train_subset = torch.utils.data.Subset(dataset_train, list(range(len(dataset_train))))\n",
    "train_subset = DivideWindowsSubset(train_subset, window_shape = 512, fill_min = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "if not os.path.exists(save_directory+\"/segment_kfold/images/train\"):\n",
    "    os.makedirs(save_directory+\"/segment_kfold/images/train\")\n",
    "\n",
    "for i in range(len(train_subset)):\n",
    "    \n",
    "    file_name = str(i+1).zfill(3)+\".png\"\n",
    "    file_path = os.path.join(save_directory+\"/segment_kfold/images/train\", file_name)\n",
    "    \n",
    "    np_array = train_subset[i][0].permute(1, 2, 0).numpy()[:,:,0]\n",
    "    \n",
    "    image = Image.fromarray((np_array*255).round().astype(np.uint8))\n",
    "    image.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/labels/train\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/labels/train\")\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/masks/train\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/masks/train\")\n",
    "    \n",
    "for i in range(len(train_subset)):\n",
    "    mask = train_subset[i][1].permute(1, 2, 0).numpy()[:,:,0]\n",
    "    mask = (mask.round() > 0).astype(np.uint8)  # Asegurarse de que la máscara sea binaria\n",
    "    \n",
    "    mask_name = str(i+1).zfill(3)+\".png\"\n",
    "    \n",
    "    labeled_mask = measure.label(mask)\n",
    "    regions = measure.regionprops(labeled_mask)\n",
    "\n",
    "    height, width = mask.shape\n",
    "    label_path = os.path.join(save_directory + \"/segment_kfold/labels/train\", mask_name.replace('.png', '.txt'))\n",
    "    with open(label_path, 'w') as f:\n",
    "        for region in regions:\n",
    "            coords = region.coords.astype(np.float32)\n",
    "            coords = coords[:, [1, 0]]  # Intercambiar columnas para tener (x, y)\n",
    "            coords[:, 0] = coords[:, 0] / width  # Normalizar coordenadas x\n",
    "            coords[:, 1] = coords[:, 1] / height  # Normalizar coordenadas y\n",
    "            coords = coords.flatten()\n",
    "            coords_str = ' '.join(map(str, coords))\n",
    "            f.write(f\"0 {coords_str}\\n\")\n",
    "    \n",
    "    mask_path = os.path.join(save_directory + \"/segment_kfold/masks/train\", mask_name)\n",
    "    mask = Image.fromarray((mask*255).round().astype(np.uint8)).convert('L')\n",
    "    mask.save(mask_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también para el de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from pnebulae_torch.preprocess import CutValues\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from skimage import exposure\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "transform_x = transforms.Compose([\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "df_test = pd.read_csv(route+\"/data_files_1c_test.csv\")\n",
    "dataset_test = NebulaeDataset(data_directory, masks_directory, df_test, transform = (transform_x, transform_y))\n",
    "\n",
    "test_subset = torch.utils.data.Subset(dataset_test, list(range(len(dataset_test))))\n",
    "test_subset = DivideWindowsSubset(test_subset, window_shape = 512, fill_min = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "if not os.path.exists(save_directory+\"/segment_kfold/images/test\"):\n",
    "    os.makedirs(save_directory+\"/segment_kfold/images/test\")\n",
    "\n",
    "for i in range(len(test_subset)):\n",
    "    \n",
    "    file_name = str(i+250).zfill(3)+\".png\"\n",
    "    file_path = os.path.join(save_directory+\"/segment_kfold/images/test\", file_name)\n",
    "    \n",
    "    np_array = test_subset[i][0].permute(1, 2, 0).numpy()[:,:,0]\n",
    "    \n",
    "    image = Image.fromarray((np_array*255).round().astype(np.uint8))\n",
    "    image.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/labels/test\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/labels/test\")\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/masks/test\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/masks/test\")\n",
    "    \n",
    "for i in range(len(test_subset)):\n",
    "    mask = test_subset[i][1].permute(1, 2, 0).numpy()[:,:,0]\n",
    "    mask = (mask.round() > 0).astype(np.uint8)  # Asegurarse de que la máscara sea binaria\n",
    "    \n",
    "    mask_name = str(i+1).zfill(3)+\".png\"\n",
    "    \n",
    "    labeled_mask = measure.label(mask)\n",
    "    regions = measure.regionprops(labeled_mask)\n",
    "\n",
    "    height, width = mask.shape\n",
    "    label_path = os.path.join(save_directory + \"/segment_kfold/labels/test\", mask_name.replace('.png', '.txt'))\n",
    "    with open(label_path, 'w') as f:\n",
    "        for region in regions:\n",
    "            coords = region.coords.astype(np.float32)\n",
    "            coords = coords[:, [1, 0]]  # Intercambiar columnas para tener (x, y)\n",
    "            coords[:, 0] = coords[:, 0] / width  # Normalizar coordenadas x\n",
    "            coords[:, 1] = coords[:, 1] / height  # Normalizar coordenadas y\n",
    "            coords = coords.flatten()\n",
    "            coords_str = ' '.join(map(str, coords))\n",
    "            f.write(f\"0 {coords_str}\\n\")\n",
    "    \n",
    "    mask_path = os.path.join(save_directory + \"/segment_kfold/masks/test\", mask_name)\n",
    "    mask = Image.fromarray((mask*255).round().astype(np.uint8)).convert('L')\n",
    "    mask.save(mask_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos las carpetas para validación si no existen y lo que se hará en cada fold será mover a estas carpetas, desde las carpetas train, todos los archivos que toquen en validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_directory + \"/segment_kfold/images/val\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/images/val\")\n",
    "    \n",
    "if not os.path.exists(save_directory + \"/segment_kfold/labels/val\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/labels/val\")\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/masks/val\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/masks/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n-seg.yaml\", task='segment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.preprocess import  CutValues\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "from lightning.pytorch import seed_everything\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "import wandb\n",
    "from wandb.integration.ultralytics import add_wandb_callback\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"21924e6e134841c5c16842c4ac42fcbe5a66feb2\"\n",
    "ruta_logs_wandb = os.environ[\"STORE\"] + \"/TFG/logs_wandb/\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "####### CONFIGURACIÓN ENTRENAMIENTO #######\n",
    "BATCH_SIZE = 124\n",
    "num_epochs = 1000\n",
    "lr = 1e-4\n",
    "window_shape = 512\n",
    "\n",
    "k = 5\n",
    "    \n",
    "############# CARGA DATASET #############\n",
    "transform_x = transforms.Compose([\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "df_train = pd.read_csv(route+\"/data_files_1c_train.csv\")\n",
    "dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))\n",
    "\n",
    "seed_everything(42, workers = True)\n",
    "\n",
    "########## ENTRENAMIENTO MODELO ##########\n",
    "# Definimos el K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state = 42)\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset_train)):\n",
    "    \n",
    "    for val_id in val_ids:\n",
    "        val_subset = torch.utils.data.Subset(dataset_train, [val_id])\n",
    "        val_subset = DivideWindowsSubset(val_subset, window_shape = window_shape, fill_min = True)\n",
    "        \n",
    "        for i in range(len(val_subset)):\n",
    "            _ = shutil.move(save_directory + \"/segment_kfold/images/train/\" + str(val_id+1+i).zfill(3) + \".png\", save_directory + \"/segment_kfold/images/val/\" + str(val_id+1+i).zfill(3) + \".png\")\n",
    "            _ = shutil.move(save_directory + \"/segment_kfold/labels/train/\" + str(val_id+1+i).zfill(3) + \".txt\", save_directory + \"/segment_kfold/labels/val/\" + str(val_id+1+i).zfill(3) + \".txt\")\n",
    "            _ = shutil.move(save_directory + \"/segment_kfold/masks/train/\" + str(val_id+1+i).zfill(3) + \".png\", save_directory + \"/segment_kfold/masks/val/\" + str(val_id+1+i).zfill(3) + \".png\")\n",
    "    \n",
    "    # Acceder a la capa de convolución inicial\n",
    "    model = YOLO(os.path.join(os.environ[\"LUSTRE\"], \"yolov8n-seg.pt\"), task='segment')\n",
    "    \n",
    "    add_wandb_callback(model, enable_model_checkpointing=False)\n",
    "    \n",
    "    # Imprimimos el fold del que van a mostrarse los resultados\n",
    "    print('--------------------------------')\n",
    "    print(f\"Model info:\\n\\t- Batch Size: {BATCH_SIZE}\\n\\t- GPUs on use: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # Entrenamos el modelo, extrayendo los resultados y guardandolos en la variable result, y evaluamos en el conjunto de test.\n",
    "    results = model.train(data = save_directory + \"/segment_kfold/segment.yaml\", epochs = num_epochs, batch = BATCH_SIZE, imgsz = window_shape, seed = 42, single_cls = True, workers = 4, cls = 0,  mask_ratio = 1, close_mosaic = num_epochs//10, verbose = False, plots = True, project = \"YOLOv8_PNeSegm\", name = 'final_test')\n",
    "\n",
    "    # testloader = torch.utils.data.DataLoader(\n",
    "    #                         dataset_test,\n",
    "    #                         batch_size=BATCH_SIZE, num_workers=8, shuffle=False, persistent_workers=True)\n",
    "    \n",
    "    # Creamos un nuevo entrenador con una sola GPU para la fase de prueba\n",
    "    # trainer_test = L.Trainer(devices = 1, strategy='auto', max_epochs=num_epochs, accelerator='cuda', log_every_n_steps=1, logger=logger_wandb, callbacks=callbacks)\n",
    "    # trainer_test.test(model, testloader)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "\n",
    "    # Nuevo código para mover archivos de vuelta a las carpetas de entrenamiento\n",
    "    val_folders = ['images', 'labels', 'masks']\n",
    "    for folder in val_folders:\n",
    "        val_path = os.path.join(save_directory, \"segment_kfold\", folder, \"val\")\n",
    "        train_path = os.path.join(save_directory, \"segment_kfold\", folder, \"train\")\n",
    "        \n",
    "        for file_name in os.listdir(val_path):\n",
    "            shutil.move(os.path.join(val_path, file_name), os.path.join(train_path, file_name))\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.preprocess import  CutValues\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "from lightning.pytorch import seed_everything\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"21924e6e134841c5c16842c4ac42fcbe5a66feb2\"\n",
    "ruta_logs_wandb = os.environ[\"STORE\"] + \"/TFG/logs_wandb/\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "####### CONFIGURACIÓN ENTRENAMIENTO #######\n",
    "BATCH_SIZE = 124\n",
    "num_epochs = 1000\n",
    "lr = 1e-4\n",
    "window_shape = 512\n",
    "\n",
    "k = 5\n",
    "    \n",
    "############# CARGA DATASET #############\n",
    "transform_x = transforms.Compose([\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "df_train = pd.read_csv(route+\"/data_files_1c_train.csv\")\n",
    "dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))\n",
    "\n",
    "seed_everything(42, workers = True)\n",
    "\n",
    "########## ENTRENAMIENTO MODELO ##########\n",
    "# Definimos el K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state = 42)\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset_train)):\n",
    "    \n",
    "    if fold == 0:\n",
    "        for val_id in val_ids:\n",
    "            val_subset = torch.utils.data.Subset(dataset_train, [val_id])\n",
    "            val_subset = DivideWindowsSubset(val_subset, window_shape = window_shape, fill_min = True)\n",
    "            \n",
    "            for i in range(len(val_subset)):\n",
    "                _ = shutil.move(save_directory + \"/segment_kfold/images/train/\" + str(val_id+1+i).zfill(3) + \".png\", save_directory + \"/segment_kfold/images/val/\" + str(val_id+1+i).zfill(3) + \".png\")\n",
    "                _ = shutil.move(save_directory + \"/segment_kfold/labels/train/\" + str(val_id+1+i).zfill(3) + \".txt\", save_directory + \"/segment_kfold/labels/val/\" + str(val_id+1+i).zfill(3) + \".txt\")\n",
    "                _ = shutil.move(save_directory + \"/segment_kfold/masks/train/\" + str(val_id+1+i).zfill(3) + \".png\", save_directory + \"/segment_kfold/masks/val/\" + str(val_id+1+i).zfill(3) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Nuevo código para mover archivos de vuelta a las carpetas de entrenamiento\n",
    "val_folders = ['images', 'labels', 'masks']\n",
    "for folder in val_folders:\n",
    "    val_path = os.path.join(save_directory, \"segment_kfold\", folder, \"val\")\n",
    "    train_path = os.path.join(save_directory, \"segment_kfold\", folder, \"train\")\n",
    "    \n",
    "    for file_name in os.listdir(val_path):\n",
    "        shutil.move(os.path.join(val_path, file_name), os.path.join(train_path, file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
