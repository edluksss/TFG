{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento personalizado de YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Vamos a cambiar el directorio de trabajo\")\n",
    "\n",
    "# Indicamos la ruta del directorio de trabajo\n",
    "route = os.getcwd()+\"/TFG/test/PNe_segmentation\"\n",
    "os.chdir(route)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\" El directorio actual es:\", current_directory)\n",
    "\n",
    "# Listamos el contenido del directorio\n",
    "files = os.listdir(current_directory)\n",
    "print(\" Contenido del directorio actual:\")\n",
    "for file in files:\n",
    "    print(\"\\t\",file)\n",
    "    \n",
    "# Listamos el contenido del directorio de las máscaras\n",
    "# masks_directory = route+\"TFG\\\\test\\\\PNe_segmentation\\\\masks\"\n",
    "# data_directory = route+\"TFG\\\\test\\\\PNe_segmentation\\\\data\"\n",
    "## Ejecución en el CESGA Finisterrae III\n",
    "masks_directory = current_directory+\"/masks\"\n",
    "data_directory = current_directory+\"/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionamos el modelo from scratch para saber la entrada y la salida que espera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Create a new YOLO model from scratch\n",
    "model = YOLO(\"yolov8n-seg.yaml\", task='segment', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificamos el modelo para que en vez de recibir 3 capas de entrada reciba uno solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a la capa de convolución inicial\n",
    "conv_layer = model.model.model[0].conv\n",
    "\n",
    "# Crear una nueva capa de convolución con 1 canal de entrada en lugar de 3\n",
    "new_conv_layer = torch.nn.Conv2d(1, conv_layer.out_channels, kernel_size=conv_layer.kernel_size, stride=conv_layer.stride, padding=conv_layer.padding, bias=conv_layer.bias is not None)\n",
    "\n",
    "# Reemplazar la capa de convolución en el modelo\n",
    "model.model.model[0].conv = new_conv_layer\n",
    "\n",
    "# Verificar la nueva configuración de la capa\n",
    "print(model.model.model[0])\n",
    "\n",
    "# # Acceder a la capa de convolución inicial\n",
    "# conv_layer = model.model.model[0].conv\n",
    "# bn_layer = model.model.model[0].bn\n",
    "# act_layer = model.model.model[0].act\n",
    "\n",
    "# # Crear una nueva capa de convolución con 1 canal de entrada en lugar de 3\n",
    "# new_conv_layer = torch.nn.Conv2d(1, 3, kernel_size=1, padding='same', bias=False)\n",
    "\n",
    "# new_first_block = torch.nn.Sequential(new_conv_layer, conv_layer, bn_layer, act_layer)\n",
    "\n",
    "# # Reemplazar la capa de convolución en el modelo\n",
    "# model.model.model[0] = new_first_block\n",
    "\n",
    "# # Verificar la nueva configuración de la capa\n",
    "# print(model.model.model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        self.f = -1\n",
    "        self.i = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_f(self):\n",
    "        return self.f\n",
    "    \n",
    "    def set_f(self, f):\n",
    "        self.f = f\n",
    "        \n",
    "model.model.model.append(MyModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya tenemos el modelo modificado, vamos a probar a cargarlo con nuestro Lightning Module personalizado para entrenarlo a nuestro manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnebulae_torch.models import smpAdapter\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "model = model.model\n",
    "conv_model = smpAdapter(model = model, learning_rate=0.0001, threshold=0.5, current_fold=0, loss_fn=smp.losses.DiceLoss, scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.preprocess import CutValues\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "transform_x = transforms.Compose([\n",
    "                    # MinMaxNorm,\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    # MinMaxImageNorm(min = -88.9933, max=125873.7500),\n",
    "                    # ApplyMorphology(operation = morphology.binary_opening, concat = True, footprint = morphology.disk(2)),\n",
    "                    # ApplyMorphology(operation = morphology.area_opening, concat = True, area_threshold = 200, connectivity = 1),\n",
    "                    # ApplyIntensityTransformation(transformation = exposure.equalize_hist, concat = True, nbins = 4096),\n",
    "                    # ApplyIntensityTransformation(transformation = exposure.equalize_adapthist, concat = True, nbins = 640, kernel_size = 5),\n",
    "                    # ApplyMorphology(operation = morphology.area_opening, concat = True, area_threshold = 200, connectivity = 1),\n",
    "                    # ApplyFilter(filter = ndimage.gaussian_filter, concat = True, sigma = 5),\n",
    "                    # transforms.ToTensor(),\n",
    "                    # CustomPad(target_size = (1984, 1984), fill_min=True, tensor_type=torch.Tensor.float)\n",
    "                    # ApplyIntensityTransformation(transformation = exposure.equalize_hist, concat = False, nbins = 256),\n",
    "                    transforms.ToTensor(),\n",
    "                    ])\n",
    "type_fnc = torch.Tensor.int\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Lambda(lambda x: type_fnc(x.round())),\n",
    "                    # CustomPad(target_size = (1984, 1984), fill = 0, tensor_type=torch.Tensor.int)\n",
    "                    ])\n",
    "\n",
    "df_train = pd.read_csv(\"data_files_1c_train.csv\")\n",
    "dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = conv_model(dataset_train[156][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(torch.sigmoid(a)[0][0].detach().numpy() > 0.55, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a intentar hacer un entrenamiento al igual que hacemos para el resto de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.preprocess import ApplyMorphology, ApplyIntensityTransformation, ApplyFilter, CustomPad, CutValues\n",
    "from pnebulae_torch.normalize import TypicalImageNorm, MinMaxImageNorm\n",
    "from pnebulae_torch.models.callbacks import PrintCallback\n",
    "from pnebulae_torch.models import basicUNet, smpAdapter, ConvNet\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "from skimage import morphology, exposure\n",
    "from scipy import ndimage\n",
    "from lightning.pytorch import seed_everything\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import wandb\n",
    "import inspect\n",
    "import time\n",
    "import gc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ########## CONFIGURACIÓN SCRIPT ##########\n",
    "    # Establecemos la clave de la API de W&B\n",
    "    os.environ[\"WANDB_API_KEY\"] = \"21924e6e134841c5c16842c4ac42fcbe5a66feb2\"\n",
    "    ruta_logs_wandb = os.environ[\"STORE\"] + \"/TFG/logs_wandb/\"\n",
    "    \n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "    ####### CONFIGURACIÓN ENTRENAMIENTO #######\n",
    "    model_name = \"YOLO_test\"\n",
    "    \n",
    "    BATCH_SIZE = 124\n",
    "    num_epochs = 2000\n",
    "    lr = 1e-2\n",
    "    window_shape = 640\n",
    "    \n",
    "    k = 5\n",
    "    \n",
    "    loss_fn = DiceLoss\n",
    "    activation_layer=torch.nn.ReLU\n",
    "    \n",
    "    if \"mode\" in inspect.signature(loss_fn).parameters:\n",
    "        type_fnc = torch.Tensor.int\n",
    "    else:\n",
    "        type_fnc = torch.Tensor.float\n",
    "        \n",
    "    ############# CARGA DATASET #############\n",
    "    transform_x = transforms.Compose([\n",
    "                        # MinMaxNorm,\n",
    "                        CutValues(factor = 2),\n",
    "                        TypicalImageNorm(factor = 1, substract=0),\n",
    "                        # MinMaxImageNorm(min = -88.9933, max=125873.7500),\n",
    "                        # ApplyMorphology(operation = morphology.binary_opening, concat = True, footprint = morphology.disk(2)),\n",
    "                        # ApplyMorphology(operation = morphology.area_opening, concat = True, area_threshold = 200, connectivity = 1),\n",
    "                        # ApplyIntensityTransformation(transformation = exposure.equalize_hist, concat = True, nbins = 4096),\n",
    "                        # ApplyIntensityTransformation(transformation = exposure.equalize_adapthist, concat = True, nbins = 640, kernel_size = 5),\n",
    "                        # ApplyMorphology(operation = morphology.area_opening, concat = True, area_threshold = 200, connectivity = 1),\n",
    "                        # ApplyFilter(filter = ndimage.gaussian_filter, concat = True, sigma = 5),\n",
    "                        # transforms.ToTensor(),\n",
    "                        # CustomPad(target_size = (1984, 1984), fill_min=True, tensor_type=torch.Tensor.float)\n",
    "                        # ApplyIntensityTransformation(transformation = exposure.equalize_hist, concat = False, nbins = 256),\n",
    "                        transforms.ToTensor(),\n",
    "                        ])\n",
    "\n",
    "    transform_y = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Lambda(lambda x: type_fnc(x.round())),\n",
    "                        # CustomPad(target_size = (1984, 1984), fill = 0, tensor_type=torch.Tensor.int)\n",
    "                        ])\n",
    "\n",
    "    df_train = pd.read_csv(\"data_files_1c_train.csv\")\n",
    "    dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))\n",
    "    \n",
    "    df_test = pd.read_csv(\"data_files_1c_test.csv\")\n",
    "    dataset_test = NebulaeDataset(data_directory, masks_directory, df_test, transform = (transform_x, transform_y))\n",
    "\n",
    "    seed_everything(42, workers = True)\n",
    "    \n",
    "    ########## ENTRENAMIENTO MODELO ##########\n",
    "    # Definimos el K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state = 42)\n",
    "    \n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset_train)):\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_loss',\n",
    "            dirpath=os.environ[\"STORE\"] + f\"/TFG/model_checkpoints/{model_name}\",\n",
    "            filename='best_model-{epoch:02d}-'+str(fold),\n",
    "            save_top_k=1,\n",
    "            mode='min',\n",
    "        )\n",
    "        \n",
    "        checkpoint_callback_last = ModelCheckpoint(\n",
    "            monitor=None,\n",
    "            dirpath=os.environ[\"STORE\"] + f\"/TFG/model_checkpoints/{model_name}\",\n",
    "            filename='last_model_fold'+str(fold),\n",
    "        )\n",
    "        \n",
    "        callbacks = [PrintCallback(), LearningRateMonitor(logging_interval='epoch'), checkpoint_callback, checkpoint_callback_last]\n",
    "        \n",
    "        # Acceder a la capa de convolución inicial\n",
    "        model = YOLO(\"yolov8n-seg.yaml\", task='segment')\n",
    "        \n",
    "        conv_layer = model.model.model[0].conv\n",
    "\n",
    "        # Crear una nueva capa de convolución con 1 canal de entrada en lugar de 3\n",
    "        new_conv_layer = torch.nn.Conv2d(dataset_train[0][0].shape[0], conv_layer.out_channels, kernel_size=conv_layer.kernel_size, stride=conv_layer.stride, padding=conv_layer.padding, bias=conv_layer.bias is not None)\n",
    "\n",
    "        # Reemplazar la capa de convolución en el modelo\n",
    "        model.model.model[0].conv = new_conv_layer\n",
    "\n",
    "        class MyModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(MyModel, self).__init__()\n",
    "                self.model = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n",
    "                    torch.nn.Upsample(scale_factor=2),\n",
    "                    torch.nn.Upsample(scale_factor=2)\n",
    "                )\n",
    "                self.f = -1\n",
    "                self.i = 0\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.model(x)\n",
    "            \n",
    "            def get_f(self):\n",
    "                return self.f\n",
    "            \n",
    "            def set_f(self, f):\n",
    "                self.f = f\n",
    "                \n",
    "        model.model.model.append(MyModel())\n",
    "        # # Acceder a la capa de convolución inicial\n",
    "        # conv_layer = model.model.model[0].conv\n",
    "        # bn_layer = model.model.model[0].bn\n",
    "        # act_layer = model.model.model[0].act\n",
    "\n",
    "        # # Crear una nueva capa de convolución con 1 canal de entrada en lugar de 3\n",
    "        # new_conv_layer = torch.nn.Conv2d(1, 3, kernel_size=1, padding='same', bias=False)\n",
    "\n",
    "        # new_first_block = torch.nn.Sequential(new_conv_layer, conv_layer, bn_layer, act_layer)\n",
    "\n",
    "        # # Reemplazar la capa de convolución en el modelo\n",
    "        # model.model.model[0] = new_first_block\n",
    "        \n",
    "        model = model.model\n",
    "        # Definimos el modelo con los pesos inicializados aleatoriamente (sin preentrenar)\n",
    "        # model = smpAdapter(model = model, learning_rate=lr, threshold=0.5, current_fold=fold, loss_fn=loss_fn, scheduler=None)\n",
    "        # model = smpAdapter(model = model, learning_rate=lr, threshold=0.5, current_fold=fold, loss_fn=loss_fn, scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau, mode='min', factor=0.1, patience=500, cooldown=150, verbose=False)\n",
    "        model = smpAdapter(model = model, learning_rate=lr, threshold=0.5, current_fold=fold, loss_fn=loss_fn, scheduler=torch.optim.lr_scheduler.StepLR, step_size = 500, gamma = 0.1, verbose=False)\n",
    "        # model = smpAdapter(model = model, learning_rate=lr, threshold=0.5, current_fold=fold, loss_fn=loss_fn, scheduler=torch.optim.lr_scheduler.MultiStepLR, milestones = [1000, 4000], gamma = 0.1, verbose=False)\n",
    "        \n",
    "        ruta_logs_wandb = os.environ[\"STORE\"] + \"/TFG/logs_wandb/\"\n",
    "        logger_wandb = WandbLogger(project=\"segmentation_TFG\", log_model = False, name=model_name, save_dir=ruta_logs_wandb)\n",
    "        logger_wandb.experiment.config.update({\"model_name\": model_name})\n",
    "\n",
    "        # log gradients, parameter histogram and model topology\n",
    "        logger_wandb.watch(model, log=\"all\")\n",
    "\n",
    "        trainer = L.Trainer(strategy='auto', max_epochs=num_epochs, accelerator='cuda', log_every_n_steps=2, logger= logger_wandb, callbacks=callbacks)\n",
    "\n",
    "        # Imprimimos el fold del que van a mostrarse los resultados\n",
    "        print('--------------------------------')\n",
    "        print(f\"Model info:\\n\\t- Batch Size: {BATCH_SIZE}\\n\\t- GPUs on use: {torch.cuda.device_count()}\")\n",
    "\n",
    "        # Creamos nuestros propios Subsets de PyTorch aplicando a cada conjunto la transformacion deseada\n",
    "        train_subset = torch.utils.data.Subset(dataset_train, train_ids)\n",
    "        val_subset = torch.utils.data.Subset(dataset_train, val_ids)\n",
    "        \n",
    "        if window_shape is not None:\n",
    "            train_subset = DivideWindowsSubset(train_subset, window_shape = window_shape, fill_min = True)\n",
    "            val_subset = DivideWindowsSubset(val_subset, window_shape = window_shape, fill_min = True)\n",
    "        \n",
    "        # Definimos un data loader por cada conjunto de datos que vamos a utilizar.\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                                train_subset,\n",
    "                                batch_size=BATCH_SIZE, num_workers=6, shuffle=True, persistent_workers=False)\n",
    "\n",
    "        valloader = torch.utils.data.DataLoader(\n",
    "                                val_subset,\n",
    "                                batch_size=BATCH_SIZE, num_workers=6, shuffle=False, persistent_workers=False)\n",
    "        \n",
    "        # Entrenamos el modelo, extrayendo los resultados y guardandolos en la variable result, y evaluamos en el conjunto de test.\n",
    "        trainer.fit(model, trainloader, valloader) \n",
    "\n",
    "        logger_wandb.experiment.unwatch(model)\n",
    "\n",
    "        # testloader = torch.utils.data.DataLoader(\n",
    "        #                         dataset_test,\n",
    "        #                         batch_size=BATCH_SIZE, num_workers=8, shuffle=False, persistent_workers=True)\n",
    "        \n",
    "        # Creamos un nuevo entrenador con una sola GPU para la fase de prueba\n",
    "        # trainer_test = L.Trainer(devices = 1, strategy='auto', max_epochs=num_epochs, accelerator='cuda', log_every_n_steps=1, logger=logger_wandb, callbacks=callbacks)\n",
    "        # trainer_test.test(model, testloader)\n",
    "\n",
    "        logger_wandb.finalize(\"success\")\n",
    "        wandb.finish()\n",
    "        \n",
    "        del model\n",
    "        del trainer\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento por defecto de YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este punto hemos conseguido entrenar el modelo YOLO, con una cabeza final personalizada y empleando nuestro método de entrenamiento, lo que vamos a proabr ahora es a entrenar YOLO tal y como viene preparado en la librería, pero vamos a seguir intentando hacer el 5Fold-CrossValidation e introduciendo las mismas imágenes que hemos probado para el resto de modelos. Para ellos lo que vamos a hacer es guardar todas las imágenes, en las ventanas de 512 respectivas y para cada fold las vamos a ir moviendo entre las diferentes carpetas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.environ['HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamos a cambiar el directorio de trabajo\n",
      " El directorio actual es: /mnt/netapp2/Home_FT2/home/ulc/co/ela/TFG/test/PNe_segmentation\n",
      " Contenido del directorio actual:\n",
      "\t train_models\n",
      "\t data\n",
      "\t create_dataset.ipynb\n",
      "\t data_files_1c.csv\n",
      "\t images\n",
      "\t image_analysis.ipynb\n",
      "\t masks\n",
      "\t segmentation_no_supervisada.ipynb\n",
      "\t segmentation_no_supervisada_2c.ipynb\n",
      "\t segmentation_no_supervisada_pytorch.ipynb\n",
      "\t segmentation_supervisada.ipynb\n",
      "\t dataset_info.csv\n",
      "\t data_files_1c_train.csv\n",
      "\t historico_notebooks\n",
      "\t data_files_1c_test.csv\n",
      "\t segmentation_no_supervisada_pytorch_clean.ipynb\n",
      "\t segmentation_supervisada_pytorch copy.ipynb\n",
      "\t segmentation_supervisada_pytorch.ipynb\n",
      "\t cesga\n",
      "\t segmentation_supervisada_pytorch copy 2.ipynb\n",
      "\t segmentation_supervisada_pytorch copy 3.ipynb\n",
      "\t data_files_1c_train_da.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Vamos a cambiar el directorio de trabajo\")\n",
    "\n",
    "# Indicamos la ruta del directorio de trabajo\n",
    "route = os.getcwd()+\"/TFG/test/PNe_segmentation\"\n",
    "os.chdir(route)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\" El directorio actual es:\", current_directory)\n",
    "\n",
    "# Listamos el contenido del directorio\n",
    "files = os.listdir(current_directory)\n",
    "print(\" Contenido del directorio actual:\")\n",
    "for file in files:\n",
    "    print(\"\\t\",file)\n",
    "    \n",
    "# Listamos el contenido del directorio de las máscaras\n",
    "# masks_directory = route+\"TFG\\\\test\\\\PNe_segmentation\\\\masks\"\n",
    "# data_directory = route+\"TFG\\\\test\\\\PNe_segmentation\\\\data\"\n",
    "## Ejecución en el CESGA Finisterrae III\n",
    "masks_directory = current_directory+\"/masks\"\n",
    "data_directory = current_directory+\"/data\"\n",
    "\n",
    "os.chdir(route+\"/../yolo_segmentation\")\n",
    "save_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir fits a JPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo realizamos para el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from pnebulae_torch.preprocess import CutValues\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from skimage import exposure\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "transform_x = transforms.Compose([\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "df_train = pd.read_csv(route+\"/data_files_1c_train.csv\")\n",
    "dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))\n",
    "\n",
    "train_subset = torch.utils.data.Subset(dataset_train, list(range(len(dataset_train))))\n",
    "train_subset = DivideWindowsSubset(train_subset, window_shape = 512, fill_min = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "if not os.path.exists(save_directory+\"/segment_kfold/images/train\"):\n",
    "    os.makedirs(save_directory+\"/segment_kfold/images/train\")\n",
    "\n",
    "for i in range(len(train_subset)):\n",
    "    \n",
    "    file_name = str(i+1).zfill(3)+\".png\"\n",
    "    file_path = os.path.join(save_directory+\"/segment_kfold/images/train\", file_name)\n",
    "    \n",
    "    np_array = train_subset[i][0].permute(1, 2, 0).numpy()[:,:,0]\n",
    "    \n",
    "    image = Image.fromarray((np_array*255).round().astype(np.uint8))\n",
    "    image.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/labels/train\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/labels/train\")\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/masks/train\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/masks/train\")\n",
    "    \n",
    "for i in range(len(train_subset)):\n",
    "    mask = train_subset[i][1].permute(1, 2, 0).numpy()[:,:,0]\n",
    "    mask = (mask.round() > 0).astype(np.uint8)  # Asegurarse de que la máscara sea binaria\n",
    "    \n",
    "    mask_name = str(i+1).zfill(3)+\".png\"\n",
    "    \n",
    "    labeled_mask = measure.label(mask)\n",
    "    regions = measure.regionprops(labeled_mask)\n",
    "\n",
    "    height, width = mask.shape\n",
    "    label_path = os.path.join(save_directory + \"/segment_kfold/labels/train\", mask_name.replace('.png', '.txt'))\n",
    "    with open(label_path, 'w') as f:\n",
    "        for region in regions:\n",
    "            coords = region.coords.astype(np.float32)\n",
    "            coords = coords[:, [1, 0]]  # Intercambiar columnas para tener (x, y)\n",
    "            coords[:, 0] = coords[:, 0] / width  # Normalizar coordenadas x\n",
    "            coords[:, 1] = coords[:, 1] / height  # Normalizar coordenadas y\n",
    "            coords = coords.flatten()\n",
    "            coords_str = ' '.join(map(str, coords))\n",
    "            f.write(f\"0 {coords_str}\\n\")\n",
    "    \n",
    "    mask_path = os.path.join(save_directory + \"/segment_kfold/masks/train\", mask_name)\n",
    "    mask = Image.fromarray((mask*255).round().astype(np.uint8)).convert('L')\n",
    "    mask.save(mask_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también para el de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from pnebulae_torch.preprocess import CutValues\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from skimage import exposure\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "transform_x = transforms.Compose([\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "df_test = pd.read_csv(route+\"/data_files_1c_test.csv\")\n",
    "dataset_test = NebulaeDataset(data_directory, masks_directory, df_test, transform = (transform_x, transform_y))\n",
    "\n",
    "test_subset = torch.utils.data.Subset(dataset_test, list(range(len(dataset_test))))\n",
    "test_subset = DivideWindowsSubset(test_subset, window_shape = 512, fill_min = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "if not os.path.exists(save_directory+\"/segment_kfold/images/test\"):\n",
    "    os.makedirs(save_directory+\"/segment_kfold/images/test\")\n",
    "\n",
    "for i in range(len(test_subset)):\n",
    "    \n",
    "    file_name = str(i+250).zfill(3)+\".png\"\n",
    "    file_path = os.path.join(save_directory+\"/segment_kfold/images/test\", file_name)\n",
    "    \n",
    "    np_array = test_subset[i][0].permute(1, 2, 0).numpy()[:,:,0]\n",
    "    \n",
    "    image = Image.fromarray((np_array*255).round().astype(np.uint8))\n",
    "    image.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/labels/test\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/labels/test\")\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/masks/test\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/masks/test\")\n",
    "    \n",
    "for i in range(len(test_subset)):\n",
    "    mask = test_subset[i][1].permute(1, 2, 0).numpy()[:,:,0]\n",
    "    mask = (mask.round() > 0).astype(np.uint8)  # Asegurarse de que la máscara sea binaria\n",
    "    \n",
    "    mask_name = str(i+250).zfill(3)+\".png\"\n",
    "    \n",
    "    labeled_mask = measure.label(mask)\n",
    "    regions = measure.regionprops(labeled_mask)\n",
    "\n",
    "    height, width = mask.shape\n",
    "    label_path = os.path.join(save_directory + \"/segment_kfold/labels/test\", mask_name.replace('.png', '.txt'))\n",
    "    with open(label_path, 'w') as f:\n",
    "        for region in regions:\n",
    "            coords = region.coords.astype(np.float32)\n",
    "            coords = coords[:, [1, 0]]  # Intercambiar columnas para tener (x, y)\n",
    "            coords[:, 0] = coords[:, 0] / width  # Normalizar coordenadas x\n",
    "            coords[:, 1] = coords[:, 1] / height  # Normalizar coordenadas y\n",
    "            coords = coords.flatten()\n",
    "            coords_str = ' '.join(map(str, coords))\n",
    "            f.write(f\"0 {coords_str}\\n\")\n",
    "    \n",
    "    mask_path = os.path.join(save_directory + \"/segment_kfold/masks/test\", mask_name)\n",
    "    mask = Image.fromarray((mask*255).round().astype(np.uint8)).convert('L')\n",
    "    mask.save(mask_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos las carpetas para validación si no existen y lo que se hará en cada fold será mover a estas carpetas, desde las carpetas train, todos los archivos que toquen en validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_directory + \"/segment_kfold/images/val\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/images/val\")\n",
    "    \n",
    "if not os.path.exists(save_directory + \"/segment_kfold/labels/val\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/labels/val\")\n",
    "\n",
    "if not os.path.exists(save_directory + \"/segment_kfold/masks/val\"):\n",
    "    os.makedirs(save_directory + \"/segment_kfold/masks/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n-seg.pt\", task='segment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/netapp2/Home_FT2/home/ulc/co/ela/TFG/test/yolo_segmentation/segment_kfold/images/train/037.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/co/ela/conda/envs/TFG_env/lib/python3.10/shutil.py:816\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/netapp2/Home_FT2/home/ulc/co/ela/TFG/test/yolo_segmentation/segment_kfold/images/train/037.png' -> '/mnt/netapp2/Home_FT2/home/ulc/co/ela/TFG/test/yolo_segmentation/segment_kfold/images/val/037.png'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m val_subset \u001b[38;5;241m=\u001b[39m DivideWindowsSubset(val_subset, window_shape \u001b[38;5;241m=\u001b[39m window_shape, fill_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val_subset)):\n\u001b[0;32m---> 59\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/segment_kfold/images/train/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_id\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzfill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/segment_kfold/images/val/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_id\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzfill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     _ \u001b[38;5;241m=\u001b[39m shutil\u001b[38;5;241m.\u001b[39mmove(save_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/segment_kfold/labels/train/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/segment_kfold/labels/val/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m     _ \u001b[38;5;241m=\u001b[39m shutil\u001b[38;5;241m.\u001b[39mmove(save_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/segment_kfold/masks/train/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/segment_kfold/masks/val/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/co/ela/conda/envs/TFG_env/lib/python3.10/shutil.py:836\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    834\u001b[0m         rmtree(src)\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 836\u001b[0m         \u001b[43mcopy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(src)\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/co/ela/conda/envs/TFG_env/lib/python3.10/shutil.py:434\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    433\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 434\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/co/ela/conda/envs/TFG_env/lib/python3.10/shutil.py:254\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    257\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/netapp2/Home_FT2/home/ulc/co/ela/TFG/test/yolo_segmentation/segment_kfold/images/train/037.png'"
     ]
    }
   ],
   "source": [
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.preprocess import  CutValues\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "from lightning.pytorch import seed_everything\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "import wandb\n",
    "from wandb.integration.ultralytics import add_wandb_callback\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"21924e6e134841c5c16842c4ac42fcbe5a66feb2\"\n",
    "ruta_logs_wandb = os.environ[\"STORE\"] + \"/TFG/logs_wandb/\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "####### CONFIGURACIÓN ENTRENAMIENTO #######\n",
    "BATCH_SIZE = 124\n",
    "num_epochs = 1000\n",
    "lr = 1e-4\n",
    "window_shape = 512\n",
    "\n",
    "k = 5\n",
    "    \n",
    "############# CARGA DATASET #############\n",
    "transform_x = transforms.Compose([\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "df_train = pd.read_csv(route+\"/data_files_1c_train.csv\")\n",
    "dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))\n",
    "\n",
    "seed_everything(42, workers = True)\n",
    "\n",
    "########## ENTRENAMIENTO MODELO ##########\n",
    "# Definimos el K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state = 42)\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset_train)):\n",
    "    \n",
    "    if fold != 1:\n",
    "        continue\n",
    "    \n",
    "    cnt = 0\n",
    "    for id in range(len(dataset_train)):\n",
    "        subset = torch.utils.data.Subset(dataset_train, [id])\n",
    "        subset = DivideWindowsSubset(subset, window_shape = window_shape, fill_min = True)\n",
    "        \n",
    "        if id in val_ids:\n",
    "            for i in range(len(subset)):\n",
    "                _ = shutil.move(save_directory + \"/segment_kfold/images/train/\" + str(id+1+i+cnt).zfill(3) + \".png\", save_directory + \"/segment_kfold/images/val/\" + str(id+1+i+cnt).zfill(3) + \".png\")\n",
    "                _ = shutil.move(save_directory + \"/segment_kfold/labels/train/\" + str(id+1+i+cnt).zfill(3) + \".txt\", save_directory + \"/segment_kfold/labels/val/\" + str(id+1+i+cnt).zfill(3) + \".txt\")\n",
    "                _ = shutil.move(save_directory + \"/segment_kfold/masks/train/\" + str(id+1+i+cnt).zfill(3) + \".png\", save_directory + \"/segment_kfold/masks/val/\" + str(id+1+i+cnt).zfill(3) + \".png\")\n",
    "        cnt += len(subset)-1\n",
    "    \n",
    "    # Acceder a la capa de convolución inicial\n",
    "    model = YOLO(\"yolov8n-seg.pt\", task='segment')\n",
    "    \n",
    "    add_wandb_callback(model, enable_model_checkpointing=False)\n",
    "    \n",
    "    # Imprimimos el fold del que van a mostrarse los resultados\n",
    "    print('--------------------------------')\n",
    "    print(f\"Model info:\\n\\t- Batch Size: {BATCH_SIZE}\\n\\t- GPUs on use: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # Entrenamos el modelo, extrayendo los resultados y guardandolos en la variable result, y evaluamos en el conjunto de test.\n",
    "    results = model.train(data = save_directory + \"/segment_kfold/segment.yaml\", \n",
    "                        pretrained = False, lr0 = 0.001, lrf = 0.01, \n",
    "                        epochs = num_epochs, batch = BATCH_SIZE, imgsz = window_shape, \n",
    "                        seed = 42, \n",
    "                        single_cls = True, \n",
    "                        workers = 8,  \n",
    "                        mask_ratio = 1, close_mosaic = num_epochs//10, \n",
    "                        verbose = False, plots = True, \n",
    "                        project = \"YOLOv8_PNeSegm\", name = f'box3_cls05_dfl1_rect_noTL_fold{fold}', \n",
    "                        patience = 0, optimizer = \"AdamW\",\n",
    "                        box = 3, cls = 0.5, dfl = 1,\n",
    "                        rect = True,\n",
    "                        save_dir = os.environ[\"STORE\"]+ \"/TFG/YOLOv8\")\n",
    "\n",
    "    # testloader = torch.utils.data.DataLoader(\n",
    "    #                         dataset_test,\n",
    "    #                         batch_size=BATCH_SIZE, num_workers=8, shuffle=False, persistent_workers=True)\n",
    "    \n",
    "    # Creamos un nuevo entrenador con una sola GPU para la fase de prueba\n",
    "    # trainer_test = L.Trainer(devices = 1, strategy='auto', max_epochs=num_epochs, accelerator='cuda', log_every_n_steps=1, logger=logger_wandb, callbacks=callbacks)\n",
    "    # trainer_test.test(model, testloader)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "\n",
    "    # Nuevo código para mover archivos de vuelta a las carpetas de entrenamiento\n",
    "    val_folders = ['images', 'labels', 'masks']\n",
    "    for folder in val_folders:\n",
    "        val_path = os.path.join(save_directory, \"segment_kfold\", folder, \"val\")\n",
    "        train_path = os.path.join(save_directory, \"segment_kfold\", folder, \"train\")\n",
    "        \n",
    "        for file_name in os.listdir(val_path):\n",
    "            shutil.move(os.path.join(val_path, file_name), os.path.join(train_path, file_name))\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from pnebulae_torch.dataset import NebulaeDataset\n",
    "from pnebulae_torch.preprocess import  CutValues\n",
    "from pnebulae_torch.normalize import TypicalImageNorm\n",
    "from pnebulae_torch.utils import DivideWindowsSubset\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "from lightning.pytorch import seed_everything\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"21924e6e134841c5c16842c4ac42fcbe5a66feb2\"\n",
    "ruta_logs_wandb = os.environ[\"STORE\"] + \"/TFG/logs_wandb/\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "####### CONFIGURACIÓN ENTRENAMIENTO #######\n",
    "BATCH_SIZE = 124\n",
    "num_epochs = 1000\n",
    "lr = 1e-4\n",
    "window_shape = 512\n",
    "\n",
    "k = 5\n",
    "    \n",
    "############# CARGA DATASET #############\n",
    "transform_x = transforms.Compose([\n",
    "                    CutValues(factor = 2),\n",
    "                    TypicalImageNorm(factor = 1, substract=0),\n",
    "                    transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "transform_y = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                    ])\n",
    "\n",
    "df_train = pd.read_csv(route+\"/data_files_1c_train.csv\")\n",
    "dataset_train = NebulaeDataset(data_directory, masks_directory, df_train, transform = (transform_x, transform_y))\n",
    "\n",
    "seed_everything(42, workers = True)\n",
    "\n",
    "########## ENTRENAMIENTO MODELO ##########\n",
    "# Definimos el K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state = 42)\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset_train)):\n",
    "    \n",
    "    if fold == 1:\n",
    "        cnt = 0\n",
    "        for id in range(len(dataset_train)):\n",
    "            subset = torch.utils.data.Subset(dataset_train, [id])\n",
    "            subset = DivideWindowsSubset(subset, window_shape = window_shape, fill_min = True)\n",
    "            \n",
    "            if id in val_ids:\n",
    "                for i in range(len(subset)):\n",
    "                    _ = shutil.move(save_directory + \"/segment_kfold/images/train/\" + str(id+1+i+cnt).zfill(3) + \".png\", save_directory + \"/segment_kfold/images/val/\" + str(id+1+i+cnt).zfill(3) + \".png\")\n",
    "                    _ = shutil.move(save_directory + \"/segment_kfold/labels/train/\" + str(id+1+i+cnt).zfill(3) + \".txt\", save_directory + \"/segment_kfold/labels/val/\" + str(id+1+i+cnt).zfill(3) + \".txt\")\n",
    "                    _ = shutil.move(save_directory + \"/segment_kfold/masks/train/\" + str(id+1+i+cnt).zfill(3) + \".png\", save_directory + \"/segment_kfold/masks/val/\" + str(id+1+i+cnt).zfill(3) + \".png\")\n",
    "            cnt += len(subset)-1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Nuevo código para mover archivos de vuelta a las carpetas de entrenamiento\n",
    "val_folders = ['images', 'labels', 'masks']\n",
    "for folder in val_folders:\n",
    "    val_path = os.path.join(save_directory, \"segment_kfold\", folder, \"val\")\n",
    "    train_path = os.path.join(save_directory, \"segment_kfold\", folder, \"train\")\n",
    "    \n",
    "    for file_name in os.listdir(val_path):\n",
    "        shutil.move(os.path.join(val_path, file_name), os.path.join(train_path, file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
